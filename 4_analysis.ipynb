{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6dae4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 23:34:13.833847: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mc4.algorithm import mc4_aggregator\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from os.path import exists\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4840a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words)\n",
    "        # Avoid zero-length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"\":\n",
    "                words[i] = \" \"\n",
    "                spaces[i] = False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1] == \" \":\n",
    "            words = words[0:-1]\n",
    "            spaces = spaces[0:-1]\n",
    "        else:\n",
    "            spaces[-1] = False\n",
    "            \n",
    "        return Doc(self.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaacbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"lemmatizer\", config={\"mode\": \"lookup\"})\n",
    "nlp.initialize()\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "nlp.max_length = 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = [\n",
    "    '1900_1909',\n",
    "    '1910_1919',\n",
    "    '1920_1929',\n",
    "    '1930_1939',\n",
    "    '1940_1949',\n",
    "    '1950_1959',\n",
    "    '1960_1969',\n",
    "    '1970_1979',\n",
    "    '1980_1989',\n",
    "    '1990_1999',\n",
    "    '2000_2009',\n",
    "    '2010_2020'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c1bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = ['china',\n",
    "                'north korea',\n",
    "                'south korea',\n",
    "                'canada',\n",
    "                'united kingdom',\n",
    "                'germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9699d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_list = ['autocracy',\n",
    "                'autocratic',\n",
    "                'dictator',\n",
    "                'dictatorship',\n",
    "               'authoritarianism',\n",
    "               'democracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa67c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_and_clean_dfs(clust_df, term_df, nlp):\n",
    "    df = pd.DataFrame()\n",
    "    df['cluster'] = clust_df.idxmax(axis=1)\n",
    "    df['prob'] = clust_df.max(axis=1)\n",
    "    df = df.merge(term_df, how='left', left_index=True, right_index=True)\n",
    "    df.top_word = df.top_word.replace(np.nan, 'and', regex=True)\n",
    "    \n",
    "    top_word = df.top_word.tolist()\n",
    "    word_weight = df.word_weight.tolist()\n",
    "    \n",
    "    doc = nlp(' '.join(top_word))\n",
    "    clean_words = [token.lemma_ if not token.is_stop and len(token)>2 else '' for token in doc]\n",
    "    clean_weights = [x if clean_words[i] != '' else 0.0 for i,x in enumerate(word_weight)]\n",
    "    df['top_word'],df['word_weight'] = clean_words, clean_weights\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e48a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_cluster_dfs(df):\n",
    "    head = 20\n",
    "    df_dict = {'full_df':df, 'clusters':{}}\n",
    "    \n",
    "    df = df.groupby('cluster').agg({'prob':'mean', 'top_word':list, 'word_weight':[list,np.max,np.argmax]})\n",
    "    df.rename(columns={'top_word': 'top_words'},inplace=True)\n",
    "    \n",
    "    top_word = []\n",
    "    top20_weights = []\n",
    "    top20_words = []\n",
    "    for index, row in df.iterrows():\n",
    "        top_word.append(row.top_words['list'][int(row.word_weight['argmax'])])\n",
    "        top20_idx = np.argsort(row.word_weight['list'])[-head:]\n",
    "        top20_weights.append([row.word_weight['list'][i] for i in top20_idx])\n",
    "        top20_words.append([row.top_words['list'][i] for i in top20_idx])        \n",
    "    df['top_word'], df['top_20_words'], df['top_20_weights'] = top_word, top20_words, top20_weights\n",
    "            \n",
    "    top_words_lists = df.top_words['list'].tolist()\n",
    "    word_weight_lists = df.word_weight['list'].tolist()\n",
    "    clusters = df.index.tolist()\n",
    "    mean_prob = df.prob['mean'].tolist()\n",
    "    \n",
    "    for i, clust in enumerate(clusters):\n",
    "        df_dict['clusters'][clust] = {}\n",
    "        df_dict['clusters'][clust]['mean_prob'] = mean_prob[i]\n",
    "        df_dict['clusters'][clust]['clust_size'] = len(top_words_lists[i])\n",
    "        \n",
    "        tdf = pd.DataFrame()\n",
    "        tdf['word'] = top_words_lists[i]\n",
    "        tdf['weight'] = word_weight_lists[i]\n",
    "        tdf = tdf.groupby('word').agg({'weight':[sum,'mean', 'count']}).reset_index()\n",
    "        tdf = tdf[tdf.word != '']\n",
    "        \n",
    "        top20_list = df.iloc[i]['top_20_words'].values[0]\n",
    "        top20_weight = df.iloc[i]['top_20_weights'].values[0]\n",
    "        top20_list = list(zip(*sorted(zip(top20_weight,top20_list), reverse=True)))[1]\n",
    "        \n",
    "        top_df = pd.DataFrame()\n",
    "        top_df['top_20_att'] = top20_list[:len(tdf)]\n",
    "        top_df['top_20_sum'] = tdf.sort_values(by = ('weight', 'sum'), ascending = False).head(head)['word'].tolist()\n",
    "        top_df['top_20_mean'] = tdf.sort_values(by = ('weight', 'mean'), ascending = False).head(head)['word'].tolist()\n",
    "        top_df['top_20_count'] = tdf.sort_values(by = ('weight', 'count'), ascending = False).head(head)['word'].tolist()\n",
    "        \n",
    "        df_dict['clusters'][clust]['top_20_df'] = top_df\n",
    "        \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6f5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_cluster_words(country_list, concept_list, year_list, nlp):\n",
    "    country_dict = {}\n",
    "    concept_dict = {}\n",
    "    concept_path_att = './crs_attention_words/concepts/'\n",
    "    concept_path_bgmm = './crs_cluster_csvs/bgmm/concepts/'\n",
    "    country_path_att = './crs_attention_words/countries/'\n",
    "    country_path_bgmm = './crs_cluster_csvs/bgmm/countries/'\n",
    "    \n",
    "    for country in country_list:\n",
    "        country_dict[country] = {}\n",
    "        for year in year_list:\n",
    "            if exists(country_path_att + country + '/' + country + '_' + year + '.csv') and exists(country_path_bgmm + country + '/' + country + '_' + year + '.csv'):\n",
    "                terms_df = pd.read_csv(country_path_att + country + '/' + country + '_' + year + '.csv')\n",
    "                bg_df = pd.read_csv(country_path_bgmm + country + '/' + country + '_' + year + '.csv')\n",
    "                df = combine_and_clean_dfs(bg_df, terms_df, nlp)\n",
    "                term_dict = gather_cluster_dfs(df)\n",
    "                country_dict[country][year] = term_dict\n",
    "            \n",
    "    for concept in concept_list:\n",
    "        concept_dict[concept] = {}\n",
    "        for year in year_list:\n",
    "            if exists(concept_path_att + concept + '/' + concept + '_' + year + '.csv') and exists(concept_path_bgmm + concept + '/' + concept + '_' + year + '.csv'):\n",
    "                terms_df = pd.read_csv(concept_path_att + concept + '/' + concept + '_' + year + '.csv')\n",
    "                bg_df = pd.read_csv(concept_path_bgmm + concept + '/' + concept + '_' + year + '.csv')\n",
    "                df = combine_and_clean_dfs(bg_df, terms_df, nlp)\n",
    "                term_dict = gather_cluster_dfs(df)\n",
    "                concept_dict[concept][year] = term_dict\n",
    "                \n",
    "    return country_dict, concept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309e2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict, concept_dict = gather_all_cluster_words(country_list, concept_list, year_list, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a3ad2-868b-4870-bf6d-a27ea4be5b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf14c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONCEPT EMBEDS\n",
    "\n",
    "with open('./crs_embeds/concept_embeds.pkl', 'rb') as handle:\n",
    "    concept_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9e042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD COUNTRY EMBEDS\n",
    "\n",
    "with open('./crs_embeds/country_embeds.pkl', 'rb') as handle:\n",
    "    country_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a9674-b50e-429b-a564-402f61e60ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfdabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cluster_embeds(term_dict, term_list, term_embeds, year_list):\n",
    "    agg_dict = {}\n",
    "    for term in term_list:\n",
    "        agg_dict[term] = {}\n",
    "        print(term)\n",
    "        for year in year_list:\n",
    "            if year in term_dict[term]:\n",
    "                agg_dict[term][year] = {}\n",
    "                df = term_dict[term][year]['full_df']\n",
    "                embed_list = term_embeds[term][year]\n",
    "                clust_nums = set(df['cluster'].tolist())\n",
    "                for clust in clust_nums:\n",
    "                    embed_idxs = df.index[df['cluster'] == clust].tolist()\n",
    "                    clust_embeds = np.array(embed_list)[embed_idxs]\n",
    "                    agg_dict[term][year][clust] = {}\n",
    "                    agg_dict[term][year][clust]['cluster_mean'] = clust_embeds.mean(axis=0)\n",
    "                \n",
    "    return agg_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c37fd772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china\n",
      "north korea\n",
      "south korea\n",
      "canada\n",
      "united kingdom\n",
      "germany\n"
     ]
    }
   ],
   "source": [
    "agg_country_clust = aggregate_cluster_embeds(country_dict, country_list, country_embeddings, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c3f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocracy\n",
      "autocratic\n",
      "dictator\n",
      "dictatorship\n",
      "authoritarianism\n",
      "democracy\n"
     ]
    }
   ],
   "source": [
    "agg_concept_clust = aggregate_cluster_embeds(concept_dict, concept_list, concept_embeddings, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a405dcc-b0fa-4a68-9b9f-fed4df059e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31b286d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_all_embeds(term_list, term_embeds, year_list):\n",
    "    agg_dict = {}\n",
    "    for term in term_list:\n",
    "        agg_dict[term] = {}\n",
    "        for year in year_list:\n",
    "            if year in term_embeds[term]:\n",
    "                agg_dict[term][year] = {}\n",
    "                embed_list = term_embeds[term][year]\n",
    "                try: \n",
    "                    #embed_list.size:\n",
    "                    agg_dict[term][year] = np.nanmean(np.array(embed_list), axis=0)\n",
    "                except:\n",
    "                    print(term + ': ' + year + ' not found')\n",
    "    return agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db25f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north korea: 1900_1909 not found\n",
      "north korea: 1910_1919 not found\n",
      "north korea: 1920_1929 not found\n",
      "north korea: 1930_1939 not found\n",
      "south korea: 1900_1909 not found\n",
      "south korea: 1910_1919 not found\n",
      "south korea: 1920_1929 not found\n",
      "south korea: 1930_1939 not found\n"
     ]
    }
   ],
   "source": [
    "agg_country = aggregate_all_embeds(country_list, country_embeddings, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e806066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictatorship: 1910_1919 not found\n",
      "authoritarianism: 1900_1909 not found\n",
      "authoritarianism: 1910_1919 not found\n"
     ]
    }
   ],
   "source": [
    "agg_concept = aggregate_all_embeds(concept_list, concept_embeddings, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b2b91-20bb-41a3-aa0b-bb90928d8513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_terms(term_dict, agg_clust_dict, term_list, year_list):\n",
    "    for term in term_list:\n",
    "        for year in year_list:\n",
    "            if year in term_dict[term]:\n",
    "                for key, value in term_dict[term][year]['clusters'].items():\n",
    "                    agg_clust_dict[term][year][key]['clust_size'] = value['clust_size']\n",
    "                    agg_clust_dict[term][year][key]['mean_prob'] = value['mean_prob']\n",
    "                    df = value['top_20_df']\n",
    "\n",
    "                    sums = df['top_20_sum'].tolist()\n",
    "                    means = df['top_20_mean'].tolist()\n",
    "                    counts = df['top_20_count'].tolist()\n",
    "\n",
    "                    all_terms = list(set(sums+means+counts))\n",
    "                    df2 = pd.DataFrame(index=all_terms, columns = ['sums','means','counts'])\n",
    "\n",
    "                    for term2 in zip(sums,means,counts):\n",
    "                        df2.loc[term2[0]]['sums'] = sums.index(term2[0])\n",
    "                        df2.loc[term2[1]]['means'] = means.index(term2[1])\n",
    "                        df2.loc[term2[2]]['counts'] = counts.index(term2[2])\n",
    "                    if len(df) > 0:\n",
    "                        agg_ranks = mc4_aggregator(df2, header_row=0, index_col=0)\n",
    "                        agg_ranks = dict(sorted(agg_ranks.items(), key=lambda item: item[1]))\n",
    "                        agg_clust_dict[term][year][key]['term_ranks'] = agg_ranks\n",
    "                    \n",
    "    return agg_clust_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d769a-1720-44fa-bc8c-a2b3a153f4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "384a8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_concept_terms = add_cluster_terms(concept_dict, agg_concept_clust, concept_list, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "664ff37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_country_terms = add_cluster_terms(country_dict, agg_country_clust, country_list, year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883ed37-aa39-4706-b126-3cb9ec63319c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2d0373e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE COUNTRY YEAR AGG\n",
    "\n",
    "with open('./crs_agg_vectors/country_year_agg.pkl', 'wb') as handle:\n",
    "    pickle.dump(agg_country, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbdea1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CONCEPT YEAR AGG\n",
    "\n",
    "with open('./crs_agg_vectors/concept_year_agg.pkl', 'wb') as handle:\n",
    "    pickle.dump(agg_concept, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e83a5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE COUNTRY CLUST AGG\n",
    "\n",
    "with open('./crs_agg_vectors/country_clust_agg.pkl', 'wb') as handle:\n",
    "    pickle.dump(agg_country_terms, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4d1998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CONCEPT CLUST AGG\n",
    "\n",
    "with open('./crs_agg_vectors/concept_clust_agg.pkl', 'wb') as handle:\n",
    "    pickle.dump(agg_concept_terms, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea41f7-4bf6-46b5-9999-a9890ca3dfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52480a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa23b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbecfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crs1]",
   "language": "python",
   "name": "conda-env-crs1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
