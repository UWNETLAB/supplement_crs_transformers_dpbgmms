{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 02:27:00.150501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "random.seed(21)\n",
    "import torch\n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT DATA\n",
    "\n",
    "### Sentences Containing Country Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = [\n",
    "    '1900_1909',\n",
    "    '1910_1919',\n",
    "    '1920_1929',\n",
    "    '1930_1939',\n",
    "    '1940_1949',\n",
    "    '1950_1959',\n",
    "    '1960_1969',\n",
    "    '1970_1979',\n",
    "    '1980_1989',\n",
    "    '1990_1999',\n",
    "    '2000_2009',\n",
    "    '2010_2020'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = ['china', 'north korea', 'south korea', 'canada', 'united kingdom', 'germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_sentence_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mask_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sents(sents, term):\n",
    "    terms = term.split(\" \")\n",
    "    sents = [sent.strip() for sent in sents]\n",
    "    mask = [all(item in sent.split(\" \") for item in terms) for sent in sents]\n",
    "    sents = np.array(sents)\n",
    "    result = sents[mask]\n",
    "    \n",
    "    return result.tolist(), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask isn't necessarily needed anymore but might be useful somehow\n",
    "def get_decade_sentences(years, term):\n",
    "    year_dict = {}\n",
    "    mask_dict = {}\n",
    "    for year in years:\n",
    "        with open('./crs_corpus/decade_' + year + '.txt') as f:\n",
    "            sents = [line for line in f.readlines() if term in line]\n",
    "            sents, mask = clean_sents(sents, term)\n",
    "            year_dict[year] = sents\n",
    "            mask_dict[year] = mask\n",
    "            \n",
    "    return year_dict, mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"China\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_china, mask_dict = get_decade_sentences(year_list, 'china')\n",
    "country_sentence_dict['china'] = sents_china\n",
    "country_mask_dict['china'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"North Korea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_north_korea, mask_dict = get_decade_sentences(year_list, 'north korea')\n",
    "country_sentence_dict['north korea'] = sents_north_korea\n",
    "country_mask_dict['north korea'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"South Korea\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_south_korea, mask_dict = get_decade_sentences(year_list, 'south korea')\n",
    "country_sentence_dict['south korea'] = sents_south_korea\n",
    "country_mask_dict['south korea'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Canada\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_canada, mask_dict = get_decade_sentences(year_list, 'canada')\n",
    "country_sentence_dict['canada'] = sents_canada\n",
    "country_mask_dict['canada'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"United Kingdom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_united_kingdom, mask_dict = get_decade_sentences(year_list, 'united kingdom')\n",
    "country_sentence_dict['united kingdom'] = sents_united_kingdom\n",
    "country_mask_dict['united kingdom'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Germany\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_germany, mask_dict = get_decade_sentences(year_list, 'germany')\n",
    "country_sentence_dict['germany'] = sents_germany\n",
    "country_mask_dict['germany'] = mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE COUNTRY MASKS\n",
    "\n",
    "with open('./crs_sents/country_sent_masks.pkl', 'wb') as handle:\n",
    "    pickle.dump(country_mask_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD COUNTRY MASKS\n",
    "\n",
    "#with open('./crs_sents/country_sent_masks.pkl', 'rb') as handle:\n",
    "#    country_mask_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE COUNTRY SENTENCES\n",
    "\n",
    "with open('./crs_sents/country_sents.pkl', 'wb') as handle:\n",
    "    pickle.dump(country_sentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD COUNTRY SENTENCES\n",
    "\n",
    "#with open('./crs_sents/country_sents.pkl', 'rb') as handle:\n",
    "#    country_sentence_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_list = ['authoritarianism',\n",
    "                'autocracy',\n",
    "                'autocratic',\n",
    "                'democracy',\n",
    "                'dictator',\n",
    "                'dictatorship']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences Containing Concepts\n",
    "\n",
    "Same thing for the following concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_sentence_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_mask_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"authoritarianism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_authoritarianism, mask_dict = get_decade_sentences(year_list, 'authoritarianism')\n",
    "concept_sentence_dict['authoritarianism'] = sents_authoritarianism\n",
    "concept_mask_dict['authoritarianism'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"autocracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_autocracy, mask_dict = get_decade_sentences(year_list, 'autocracy')\n",
    "concept_sentence_dict['autocracy'] = sents_autocracy\n",
    "concept_mask_dict['autocracy'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"autocratic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_autocratic, mask_dict = get_decade_sentences(year_list, 'autocratic')\n",
    "concept_sentence_dict['autocratic'] = sents_autocratic\n",
    "concept_mask_dict['autocratic'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"democracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_democracy, mask_dict = get_decade_sentences(year_list, 'democracy')\n",
    "concept_sentence_dict['democracy'] = sents_democracy\n",
    "concept_mask_dict['democracy'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"dictator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dictator, mask_dict = get_decade_sentences(year_list, 'dictator')\n",
    "concept_sentence_dict['dictator'] = sents_dictator\n",
    "concept_mask_dict['dictator'] = mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"dictatorship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dictatorship, mask_dict = get_decade_sentences(year_list, 'dictatorship')\n",
    "concept_sentence_dict['dictatorship'] = sents_dictatorship\n",
    "concept_mask_dict['dictatorship'] = mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CONCEPT SENTENCE MASKS\n",
    "\n",
    "with open('./crs_sents/concept_sent_masks.pkl', 'wb') as handle:\n",
    "    pickle.dump(concept_mask_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONCEPT SENTENCE MASKS\n",
    "\n",
    "#with open('./crs_sents/concept_sent_masks.pkl', 'rb') as handle:\n",
    "#    concept_mask_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CONCEPT SENTENCES\n",
    "\n",
    "with open('./crs_sents/concept_sents.pkl', 'wb') as handle:\n",
    "    pickle.dump(concept_sentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONCEPT SENTENCES\n",
    "\n",
    "#with open('./crs_sents/concept_sents.pkl', 'rb') as handle:\n",
    "#    concept_sentence_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS & FUNCTIONS\n",
    "\n",
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [-3, -2, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_idx(sent: str, word: str):\n",
    "    if ' ' in word:\n",
    "        word = word.split(' ')\n",
    "        sent = sent.split(' ')\n",
    "        idx_list = [sent.index(x) for x in word]\n",
    "        return idx_list\n",
    "    else:\n",
    "        return sent.split(\" \").index(word)\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "       Select only those subword token outputs that belong to our word of interest\n",
    "       and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n",
    "\n",
    "\n",
    "def get_word_vector(sent, idx, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "       that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, truncation=True, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    \n",
    "    if isinstance(idx, list):\n",
    "\n",
    "        token_ids_word = np.where(np.isin(np.array(encoded.word_ids()), (idx)))\n",
    "    else:\n",
    "        token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, token_ids_word, model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_decade_model(year):\n",
    "    config = DistilBertConfig.from_json_file('./crs_models/' + year + '/config.json')\n",
    "    config.output_hidden_states = True\n",
    "    model = DistilBertForMaskedLM(config)\n",
    "    model.load_state_dict(torch.load('./crs_models/' + year + '/pytorch_model.bin', map_location = torch.device(\"cuda\")))\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./crs_models/' + year + '/')\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(sent, term, tokenizer, model, layers):\n",
    "    idx = get_word_idx(sent, term)\n",
    "    term_tensor = get_word_vector(sent, idx, tokenizer, model, layers)\n",
    "    return term_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer needed but useful for the future\n",
    "def ensure_spaces(sent, term):\n",
    "    sent = re.sub(rf'(?<![ ])({term})', r' \\1', sent)\n",
    "    sent = re.sub(rf'({term})(?![ ])', r'\\1 ', sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_from_sents(year_list, term, sentence_dict, layers, random_sample = True):\n",
    "    \n",
    "    vector_dict = {}\n",
    "    \n",
    "    if random_sample == True:\n",
    "        shortest_decade = min([len(sentence_dict[term][x]) for x in sentence_dict[term]])\n",
    "    print(\"Starting \" + term + \" at \" + str(datetime.now()))    \n",
    "    for year in year_list:\n",
    "        vector_list = []\n",
    "        \n",
    "        sents = sentence_dict[term][year]\n",
    "        \n",
    "        if len(sents) == 0:\n",
    "            vector_dict[year] = None\n",
    "        else:\n",
    "        \n",
    "            year_model, year_tokenizer = load_decade_model(year)\n",
    "\n",
    "\n",
    "            if random_sample == True:\n",
    "                sents = random.sample(sents,shortest_decade)\n",
    "\n",
    "            for sent in sents:\n",
    "                vector = extract_embedding(sent, term, year_tokenizer, year_model, layers)\n",
    "                vector_list.append(vector)\n",
    "\n",
    "            vector_list = [v.detach().cpu().numpy() for v in vector_list]    \n",
    "            vector_dict[year] = vector_list\n",
    "        print(\"Finished \" + year + \" at \" + str(datetime.now()))\n",
    "    return vector_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CONTEXTUAL EMBEDDINGS FOR COUNTRIES\n",
    "\n",
    "### China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting china at 2023-06-16 02:27:41.689159\n",
      "Finished 1900_1909 at 2023-06-16 02:27:47.989825\n",
      "Finished 1910_1919 at 2023-06-16 02:27:53.653754\n",
      "Finished 1920_1929 at 2023-06-16 02:27:59.841856\n",
      "Finished 1930_1939 at 2023-06-16 02:28:07.295891\n",
      "Finished 1940_1949 at 2023-06-16 02:28:21.201747\n",
      "Finished 1950_1959 at 2023-06-16 02:28:39.563573\n",
      "Finished 1960_1969 at 2023-06-16 02:29:02.294407\n",
      "Finished 1970_1979 at 2023-06-16 02:29:29.859733\n",
      "Finished 1980_1989 at 2023-06-16 02:30:06.590811\n",
      "Finished 1990_1999 at 2023-06-16 02:31:27.322137\n",
      "Finished 2000_2009 at 2023-06-16 02:33:40.693193\n",
      "Finished 2010_2020 at 2023-06-16 02:36:00.336003\n"
     ]
    }
   ],
   "source": [
    "te_china = get_embeddings_from_sents(year_list, 'china', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['china'] = te_china"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting north korea at 2023-06-16 02:36:00.365076\n",
      "Finished 1900_1909 at 2023-06-16 02:36:00.365211\n",
      "Finished 1910_1919 at 2023-06-16 02:36:00.365231\n",
      "Finished 1920_1929 at 2023-06-16 02:36:00.365255\n",
      "Finished 1930_1939 at 2023-06-16 02:36:00.365272\n",
      "Finished 1940_1949 at 2023-06-16 02:36:03.410637\n",
      "Finished 1950_1959 at 2023-06-16 02:36:06.692288\n",
      "Finished 1960_1969 at 2023-06-16 02:36:10.075570\n",
      "Finished 1970_1979 at 2023-06-16 02:36:13.789492\n",
      "Finished 1980_1989 at 2023-06-16 02:36:18.320511\n",
      "Finished 1990_1999 at 2023-06-16 02:36:26.019112\n",
      "Finished 2000_2009 at 2023-06-16 02:36:41.032234\n",
      "Finished 2010_2020 at 2023-06-16 02:36:55.984015\n"
     ]
    }
   ],
   "source": [
    "te_north_korea = get_embeddings_from_sents(year_list, 'north korea', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['north korea'] = te_north_korea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting south korea at 2023-06-16 02:36:56.007715\n",
      "Finished 1900_1909 at 2023-06-16 02:36:56.007765\n",
      "Finished 1910_1919 at 2023-06-16 02:36:56.007779\n",
      "Finished 1920_1929 at 2023-06-16 02:36:56.007789\n",
      "Finished 1930_1939 at 2023-06-16 02:36:56.007798\n",
      "Finished 1940_1949 at 2023-06-16 02:36:59.038216\n",
      "Finished 1950_1959 at 2023-06-16 02:37:02.451884\n",
      "Finished 1960_1969 at 2023-06-16 02:37:05.759869\n",
      "Finished 1970_1979 at 2023-06-16 02:37:10.137698\n",
      "Finished 1980_1989 at 2023-06-16 02:37:16.547567\n",
      "Finished 1990_1999 at 2023-06-16 02:37:28.247878\n",
      "Finished 2000_2009 at 2023-06-16 02:37:43.641695\n",
      "Finished 2010_2020 at 2023-06-16 02:38:00.088417\n"
     ]
    }
   ],
   "source": [
    "te_south_korea = get_embeddings_from_sents(year_list, 'south korea', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['south korea'] = te_south_korea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting canada at 2023-06-16 02:38:00.115306\n",
      "Finished 1900_1909 at 2023-06-16 02:38:03.437875\n",
      "Finished 1910_1919 at 2023-06-16 02:38:08.227189\n",
      "Finished 1920_1929 at 2023-06-16 02:38:13.448247\n",
      "Finished 1930_1939 at 2023-06-16 02:38:20.504024\n",
      "Finished 1940_1949 at 2023-06-16 02:38:33.083437\n",
      "Finished 1950_1959 at 2023-06-16 02:38:42.510285\n",
      "Finished 1960_1969 at 2023-06-16 02:38:52.874067\n",
      "Finished 1970_1979 at 2023-06-16 02:39:10.096580\n",
      "Finished 1980_1989 at 2023-06-16 02:39:33.736903\n",
      "Finished 1990_1999 at 2023-06-16 02:40:17.542112\n",
      "Finished 2000_2009 at 2023-06-16 02:41:23.913936\n",
      "Finished 2010_2020 at 2023-06-16 02:42:21.809694\n"
     ]
    }
   ],
   "source": [
    "te_canada = get_embeddings_from_sents(year_list, 'canada', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['canada'] = te_canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### United Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting united kingdom at 2023-06-16 02:42:21.844634\n",
      "Finished 1900_1909 at 2023-06-16 02:42:25.554399\n",
      "Finished 1910_1919 at 2023-06-16 02:42:29.319631\n",
      "Finished 1920_1929 at 2023-06-16 02:42:33.021176\n",
      "Finished 1930_1939 at 2023-06-16 02:42:36.865201\n",
      "Finished 1940_1949 at 2023-06-16 02:42:41.364643\n",
      "Finished 1950_1959 at 2023-06-16 02:42:48.016286\n",
      "Finished 1960_1969 at 2023-06-16 02:42:55.410034\n",
      "Finished 1970_1979 at 2023-06-16 02:43:02.476876\n",
      "Finished 1980_1989 at 2023-06-16 02:43:12.192708\n",
      "Finished 1990_1999 at 2023-06-16 02:43:29.849656\n",
      "Finished 2000_2009 at 2023-06-16 02:43:57.871632\n",
      "Finished 2010_2020 at 2023-06-16 02:44:21.888468\n"
     ]
    }
   ],
   "source": [
    "te_united_kingdom = get_embeddings_from_sents(year_list, 'united kingdom', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['united kingdom'] = te_united_kingdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting germany at 2023-06-16 02:44:21.921232\n",
      "Finished 1900_1909 at 2023-06-16 02:44:26.983645\n",
      "Finished 1910_1919 at 2023-06-16 02:44:38.984978\n",
      "Finished 1920_1929 at 2023-06-16 02:44:48.991448\n",
      "Finished 1930_1939 at 2023-06-16 02:45:09.779740\n",
      "Finished 1940_1949 at 2023-06-16 02:45:42.425888\n",
      "Finished 1950_1959 at 2023-06-16 02:46:09.365998\n",
      "Finished 1960_1969 at 2023-06-16 02:46:35.897531\n",
      "Finished 1970_1979 at 2023-06-16 02:47:13.586993\n",
      "Finished 1980_1989 at 2023-06-16 02:48:08.496424\n",
      "Finished 1990_1999 at 2023-06-16 02:50:18.177340\n",
      "Finished 2000_2009 at 2023-06-16 02:52:07.407327\n",
      "Finished 2010_2020 at 2023-06-16 02:53:38.446727\n"
     ]
    }
   ],
   "source": [
    "te_germany = get_embeddings_from_sents(year_list, 'germany', country_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_embeddings['germany'] = te_germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canada\n",
      "1900_1909\n",
      "18\n",
      "canada\n",
      "1910_1919\n",
      "66\n",
      "canada\n",
      "1920_1929\n",
      "82\n",
      "canada\n",
      "1930_1939\n",
      "149\n",
      "canada\n",
      "1940_1949\n",
      "318\n",
      "canada\n",
      "1950_1959\n",
      "222\n",
      "canada\n",
      "1960_1969\n",
      "253\n",
      "canada\n",
      "1970_1979\n",
      "537\n",
      "canada\n",
      "1980_1989\n",
      "721\n",
      "canada\n",
      "1990_1999\n",
      "1502\n",
      "canada\n",
      "2000_2009\n",
      "1769\n",
      "canada\n",
      "2010_2020\n",
      "1300\n",
      "united kingdom\n",
      "1900_1909\n",
      "17\n",
      "united kingdom\n",
      "1910_1919\n",
      "10\n",
      "united kingdom\n",
      "1920_1929\n",
      "9\n",
      "united kingdom\n",
      "1930_1939\n",
      "17\n",
      "united kingdom\n",
      "1940_1949\n",
      "39\n",
      "united kingdom\n",
      "1950_1959\n",
      "80\n",
      "united kingdom\n",
      "1960_1969\n",
      "92\n",
      "united kingdom\n",
      "1970_1979\n",
      "88\n",
      "united kingdom\n",
      "1980_1989\n",
      "153\n",
      "united kingdom\n",
      "1990_1999\n",
      "324\n",
      "united kingdom\n",
      "2000_2009\n",
      "532\n",
      "united kingdom\n",
      "2010_2020\n",
      "454\n",
      "germany\n",
      "1900_1909\n",
      "39\n",
      "germany\n",
      "1910_1919\n",
      "183\n",
      "germany\n",
      "1920_1929\n",
      "152\n",
      "germany\n",
      "1930_1939\n",
      "387\n",
      "germany\n",
      "1940_1949\n",
      "630\n",
      "germany\n",
      "1950_1959\n",
      "530\n",
      "germany\n",
      "1960_1969\n",
      "507\n",
      "germany\n",
      "1970_1979\n",
      "764\n",
      "germany\n",
      "1980_1989\n",
      "1124\n",
      "germany\n",
      "1990_1999\n",
      "2761\n",
      "germany\n",
      "2000_2009\n",
      "2310\n",
      "germany\n",
      "2010_2020\n",
      "1952\n",
      "north korea\n",
      "1940_1949\n",
      "2\n",
      "north korea\n",
      "1950_1959\n",
      "13\n",
      "north korea\n",
      "1960_1969\n",
      "17\n",
      "north korea\n",
      "1970_1979\n",
      "28\n",
      "north korea\n",
      "1980_1989\n",
      "57\n",
      "north korea\n",
      "1990_1999\n",
      "170\n",
      "north korea\n",
      "2000_2009\n",
      "403\n",
      "north korea\n",
      "2010_2020\n",
      "350\n",
      "south korea\n",
      "1940_1949\n",
      "7\n",
      "south korea\n",
      "1950_1959\n",
      "16\n",
      "south korea\n",
      "1960_1969\n",
      "16\n",
      "south korea\n",
      "1970_1979\n",
      "49\n",
      "south korea\n",
      "1980_1989\n",
      "117\n",
      "south korea\n",
      "1990_1999\n",
      "298\n",
      "south korea\n",
      "2000_2009\n",
      "428\n",
      "south korea\n",
      "2010_2020\n",
      "466\n"
     ]
    }
   ],
   "source": [
    "# Not every country appears in every decade\n",
    "\n",
    "for country in ['canada', 'united kingdom', 'germany', 'north korea', 'south korea']:\n",
    "    for year in year_list:\n",
    "        if country_embeddings[country][year] != None:\n",
    "            print(country)\n",
    "            print(year)\n",
    "            print(len(country_embeddings[country][year]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE COUNTRY EMBEDDINGS\n",
    "\n",
    "with open('./crs_embeds/country_embeds.pkl', 'wb') as handle:\n",
    "    pickle.dump(country_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD COUNTRY EMBEDDINGS\n",
    "\n",
    "#with open('./crs_embeds/country_embeds.pkl', 'rb') as handle:\n",
    "#    country_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (CONTEXTUAL) EMBEDDINGS FOR THEORETICAL CONCEPTS\n",
    "\n",
    "Same thing for the following concepts listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### \"authoritarianism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting authoritarianism at 2023-06-16 04:44:32.040125\n",
      "Finished 1900_1909 at 2023-06-16 04:44:32.040240\n",
      "Finished 1910_1919 at 2023-06-16 04:44:32.040273\n",
      "Finished 1920_1929 at 2023-06-16 04:44:35.158662\n",
      "Finished 1930_1939 at 2023-06-16 04:44:38.260987\n",
      "Finished 1940_1949 at 2023-06-16 04:44:41.662581\n",
      "Finished 1950_1959 at 2023-06-16 04:44:45.289243\n",
      "Finished 1960_1969 at 2023-06-16 04:44:49.385495\n",
      "Finished 1970_1979 at 2023-06-16 04:44:54.044396\n",
      "Finished 1980_1989 at 2023-06-16 04:45:00.592283\n",
      "Finished 1990_1999 at 2023-06-16 04:45:12.047924\n",
      "Finished 2000_2009 at 2023-06-16 04:45:22.272721\n",
      "Finished 2010_2020 at 2023-06-16 04:45:32.172229\n"
     ]
    }
   ],
   "source": [
    "te_authoritarianism = get_embeddings_from_sents(year_list, 'authoritarianism', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['authoritarianism'] = te_authoritarianism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"autocracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autocracy at 2023-06-16 04:46:48.467546\n",
      "Finished 1900_1909 at 2023-06-16 04:46:51.631813\n",
      "Finished 1910_1919 at 2023-06-16 04:46:55.496677\n",
      "Finished 1920_1929 at 2023-06-16 04:46:58.834558\n",
      "Finished 1930_1939 at 2023-06-16 04:47:01.940681\n",
      "Finished 1940_1949 at 2023-06-16 04:47:05.198069\n",
      "Finished 1950_1959 at 2023-06-16 04:47:08.733583\n",
      "Finished 1960_1969 at 2023-06-16 04:47:12.359848\n",
      "Finished 1970_1979 at 2023-06-16 04:47:15.954734\n",
      "Finished 1980_1989 at 2023-06-16 04:47:19.987169\n",
      "Finished 1990_1999 at 2023-06-16 04:47:25.271506\n",
      "Finished 2000_2009 at 2023-06-16 04:47:31.822263\n",
      "Finished 2010_2020 at 2023-06-16 04:47:37.896337\n"
     ]
    }
   ],
   "source": [
    "te_autocracy = get_embeddings_from_sents(year_list, 'autocracy', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['autocracy'] = te_autocracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"autocratic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autocratic at 2023-06-16 04:47:37.916589\n",
      "Finished 1900_1909 at 2023-06-16 04:47:40.703190\n",
      "Finished 1910_1919 at 2023-06-16 04:47:43.829769\n",
      "Finished 1920_1929 at 2023-06-16 04:47:46.941488\n",
      "Finished 1930_1939 at 2023-06-16 04:47:50.060274\n",
      "Finished 1940_1949 at 2023-06-16 04:47:53.333952\n",
      "Finished 1950_1959 at 2023-06-16 04:47:56.756155\n",
      "Finished 1960_1969 at 2023-06-16 04:48:00.534103\n",
      "Finished 1970_1979 at 2023-06-16 04:48:04.515920\n",
      "Finished 1980_1989 at 2023-06-16 04:48:08.503779\n",
      "Finished 1990_1999 at 2023-06-16 04:48:15.112008\n",
      "Finished 2000_2009 at 2023-06-16 04:48:21.944000\n",
      "Finished 2010_2020 at 2023-06-16 04:48:30.215912\n"
     ]
    }
   ],
   "source": [
    "te_autocratic = get_embeddings_from_sents(year_list, 'autocratic', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['autocratic'] = te_autocratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"democracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting democracy at 2023-06-16 04:48:30.240336\n",
      "Finished 1900_1909 at 2023-06-16 04:48:36.482727\n",
      "Finished 1910_1919 at 2023-06-16 04:48:50.822462\n",
      "Finished 1920_1929 at 2023-06-16 04:49:03.507865\n",
      "Finished 1930_1939 at 2023-06-16 04:49:24.411149\n",
      "Finished 1940_1949 at 2023-06-16 04:50:09.755148\n",
      "Finished 1950_1959 at 2023-06-16 04:50:55.333172\n",
      "Finished 1960_1969 at 2023-06-16 04:51:43.487250\n",
      "Finished 1970_1979 at 2023-06-16 04:52:40.103805\n",
      "Finished 1980_1989 at 2023-06-16 04:54:29.656739\n",
      "Finished 1990_1999 at 2023-06-16 04:59:22.490105\n",
      "Finished 2000_2009 at 2023-06-16 05:06:49.165024\n",
      "Finished 2010_2020 at 2023-06-16 05:12:04.832059\n"
     ]
    }
   ],
   "source": [
    "te_democracy = get_embeddings_from_sents(year_list, 'democracy', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['democracy'] = te_democracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"dictator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dictator at 2023-06-16 05:12:04.868383\n",
      "Finished 1900_1909 at 2023-06-16 05:12:08.191754\n",
      "Finished 1910_1919 at 2023-06-16 05:12:11.743348\n",
      "Finished 1920_1929 at 2023-06-16 05:12:15.196074\n",
      "Finished 1930_1939 at 2023-06-16 05:12:19.773666\n",
      "Finished 1940_1949 at 2023-06-16 05:12:24.253918\n",
      "Finished 1950_1959 at 2023-06-16 05:12:29.041182\n",
      "Finished 1960_1969 at 2023-06-16 05:12:33.439560\n",
      "Finished 1970_1979 at 2023-06-16 05:12:38.351999\n",
      "Finished 1980_1989 at 2023-06-16 05:12:44.779669\n",
      "Finished 1990_1999 at 2023-06-16 05:12:52.008237\n",
      "Finished 2000_2009 at 2023-06-16 05:13:01.639092\n",
      "Finished 2010_2020 at 2023-06-16 05:13:08.946092\n"
     ]
    }
   ],
   "source": [
    "te_dictator = get_embeddings_from_sents(year_list, 'dictator', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['dictator'] = te_dictator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"dictatorship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dictatorship at 2023-06-16 05:13:08.978021\n",
      "Finished 1900_1909 at 2023-06-16 05:13:12.072556\n",
      "Finished 1910_1919 at 2023-06-16 05:13:12.072778\n",
      "Finished 1920_1929 at 2023-06-16 05:13:16.101846\n",
      "Finished 1930_1939 at 2023-06-16 05:13:22.614184\n",
      "Finished 1940_1949 at 2023-06-16 05:13:29.188358\n",
      "Finished 1950_1959 at 2023-06-16 05:13:36.993360\n",
      "Finished 1960_1969 at 2023-06-16 05:13:45.506059\n",
      "Finished 1970_1979 at 2023-06-16 05:13:54.684327\n",
      "Finished 1980_1989 at 2023-06-16 05:14:08.755251\n",
      "Finished 1990_1999 at 2023-06-16 05:14:29.539421\n",
      "Finished 2000_2009 at 2023-06-16 05:14:51.459365\n",
      "Finished 2010_2020 at 2023-06-16 05:15:10.832965\n"
     ]
    }
   ],
   "source": [
    "te_dictatorship = get_embeddings_from_sents(year_list, 'dictatorship', concept_sentence_dict, layers, random_sample = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_embeddings['dictatorship'] = te_dictatorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CONCEPT EMBEDDINGS\n",
    "\n",
    "with open('./crs_embeds/concept_embeds.pkl', 'wb') as handle:\n",
    "    pickle.dump(concept_embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONCEPT EMBEDDINGS\n",
    "\n",
    "#with open('./crs_embeds/concept_embeds.pkl', 'rb') as handle:\n",
    "#    concept_embeddings = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crs1]",
   "language": "python",
   "name": "conda-env-crs1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "refs.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
